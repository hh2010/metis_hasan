{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Jonathan Balaban\n",
    "\n",
    "Credits and thanks to:\n",
    "- [Stephen Welch](https://github.com/stephencwelch/Neural-Networks-Demystified)\n",
    "- [Harsh Pokharna](https://medium.com/technologymadeeasy/for-dummies-the-introduction-to-neural-networks-we-all-need-c50f6012d5eb#.93dgf0vg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANNs, also connectionist systems) are a computational approach that mimics brain function: a large collection of linked neural units. Each neural unit is connected with many others, and links can be enforcing or inhibitory in their effect on the activation state of connected neural units. Each individual neural unit may have a summation function which combines the values of all its inputs together. There may be a threshold function or limiting function on each connection and on the unit itself: such that the signal must surpass the limit before propagating to other neurons. These systems are self-learning and trained, rather than explicitly programmed, and excel in areas where the solution or feature detection is difficult to express in a traditional computer program.\n",
    "\n",
    "![Neuron](https://cdn-images-1.medium.com/max/1600/1*MnmwgNzk5YkMhC3Ttb09SQ.jpeg)\n",
    "\n",
    "Neural networks typically consist of multiple layers or a cube design, and the signal path traverses from front to back. Back propagation is where the forward stimulation is used to reset weights on the \"front\" neural units and this is sometimes done in combination with training where the correct result is known. More modern networks are a bit more free flowing in terms of stimulation and inhibition with connections interacting in a much more chaotic and complex fashion.\n",
    "\n",
    "Dynamic neural networks are the most advanced- in that they dynamically can, based on rules, form new connections and even new neural units while disabling others (similar to regularization). ANNs typically work with a few thousand to a few million neural units and millions of connections, which is still several orders of magnitude less complex than the human brain and closer to the computing power of an insect.\n",
    "\n",
    "![Perceptron Neural Net](https://cdn-images-1.medium.com/max/1600/1*nRRXhhjSjKNpGn-T3yF2Ew.jpeg)\n",
    "_A perceptron is the digital equivalent of a neuron, firing if strength of inputs exceeds its threshold `theta`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/296px-Colored_neural_network.svg.png)\n",
    "_General Neural Network with Hidden Layer_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives and Gradient Descent\n",
    "Because of the massive complexity of ANNs, we almost never have a \"perfect\" analytic solution and must resort to optimizing our weights. Weights are the unknown scalar values on branches/links/connections that amplify or reduce the relative strength of one input over another. So, to truly understand how ANNs are built and solved, let's review some calculus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot y = x-squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = x^2. A lovely parabola! But how do we determine its rate of change? This skill will allow us to find a cost function's minimum and optimize our ANN weights. If your answer is to take the derivative of y with respect to x (dy/dx), and find y = x2, you would be correct. But what if computers don't know calculus? What if they were to approximate the change in y based on a tiny change in x, called epsilon?\n",
    "\n",
    "We need to be careful not to make our denominator too small! We will lose precision, and a zero value would, of course, blow up our laptops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create our function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate delta y / delta x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare with our known calculus solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic, so computers can mimic calculus to find local (hopefully global) minima. Keep in mind that gradient descent has its limits. You won't necessarily find a global minimum, a great solution in n iterations, or any solution at all, depending on your function. Yet, if we tried a brute-force \"gridsearch\" method by checking values, we would get smashed by THE CURSE OF DIMENSIONALITY!!\n",
    "\n",
    "Imagine a simple ANN with 3x4 input to hidden connections per the image above. Imagine we wanted to try 100 possible values for each. And imagine our fast computers could compute 100 calculations in one millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total time calculation in years\n",
    "total = 100**12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#forgetthis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build your own neural net!\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        # hyperparameters, or structure of our network\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        # weights, parameters to solve for\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        # sigmoid activation function\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        # derivative of sigmoid to optimize\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        # compute cost for given X,y, use weights already stored in class\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # compute derivative with respect to W and W2 for a given X and y\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNs in Sklearn\n",
    "Today, we'll take a look at [Multi-layer Perceptron (MLP)](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) models in sklearn. [This guide on perceptrons](http://www.cprogramming.com/tutorial/AI/perceptron.html) might be helpful.\n",
    "\n",
    "The advantages of MLP are:\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using partial_fit.\n",
    "\n",
    "The disadvantages of MLP include:\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build simple neural net with sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X = [[0., 0.], [1., 1.], [1., 0.]]\n",
    "y = [0, 1, 1]\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(5,2), solver='lbfgs')\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict new observations\n",
    "clf.predict([[0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf.coefs_ contains the weight matrices that constitute our model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 5), (5, 2), (2, 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[  1.09542421e-03,   1.99662861e-03,   1.62249937e+00,\n",
       "          -2.03246470e+00,  -4.73470028e-02],\n",
       "        [  2.84597711e-03,   1.61587452e-03,   4.99417159e-02,\n",
       "          -9.37065139e-02,  -2.33447380e-02]]),\n",
       " array([[  5.48593070e-03,   3.73651668e-03],\n",
       "        [  3.00633223e-03,  -3.47384967e-03],\n",
       "        [  3.12619088e-03,  -1.47910849e+00],\n",
       "        [ -1.89205740e-03,   2.43003446e+00],\n",
       "        [ -5.16844163e-03,   6.40558372e-01]]),\n",
       " array([[-0.00609055],\n",
       "        [-2.75765493]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find parameters\n",
    "print([coef.shape for coef in clf.coefs_])\n",
    "clf.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.91091071, -0.92073416,  2.79999024,  3.54706393,  1.05658564]),\n",
       " array([-0.61208439,  2.47748849]),\n",
       " array([ 11.23843769])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find intercepts\n",
    "clf.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.71866410e-05,   9.99972813e-01],\n",
       "       [  9.99868438e-01,   1.31562105e-04]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict probabilities\n",
    "clf.predict_proba([[1,0],[0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPs work well with multiclass problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=15, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='lbfgs', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create MPL with alpha=1e-5, lbfgs solver, and one hidden layer\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [[0, 1], [1, 1]]\n",
    "              \n",
    "clf = MLPClassifier(solver='lbfgs', alpha = 1e-5, hidden_layer_sizes=(15))\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict two observations\n",
    "clf.predict([[0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "The `MLPRegressor` class implements an MLP that trains using backpropagation with no activation function in the output layer, which can also be seen as using the identity function as activation function. Therefore, it uses the square error as the loss function, and the output is a set of continuous values. Amazingly, `MLPRegressor` also supports multi-output regression, in which a sample can have more than one target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and General Tips\n",
    "\n",
    "Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. For example, normalize X to [0, 1] or [-1, +1], or standardize. Note: that you must apply the same scaling (but not at the same time) to the test set for meaningful results!!\n",
    "\n",
    "Finding a reasonable regularization parameter \\alpha is best done using GridSearchCV, usually in the range `10.0 ** -np.arange(1, 7)`\n",
    "\n",
    "L-BFGS converges faster and with better solutions on small datasets. For relatively large datasets, Adam is performant and robust. SGD with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
