{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "## What is Spark?\n",
    "\n",
    "* Spark is a fast and general engine for large-scale distributed data processing.\n",
    "* Spark can run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.\n",
    "* Spark improves on MapReduce's computation model with an advanced DAG (Directed Acyclic Graph) execution engine that supports cyclic data flow and in-memory computing.\n",
    "* Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.\n",
    "* Spark is written in Scala, but has bindings for Java, Python (PySpark), and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark modules\n",
    "\n",
    "Spark has several modules:\n",
    "\n",
    "* **Spark SQL**: Interact with Spark via SQL interface (like Hive for Hadoop)\n",
    "* **Spark Streaming**: Handle streaming data for realtime workflows\n",
    "* **Spark MLLib**: Machine Learning in Spark (think sklearn for big data)\n",
    "* **Spark GraphX**: Graph processing in Spark\n",
    "\n",
    "<img src='image/spark_stack.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark concepts\n",
    "\n",
    "There are four concepts that are key to understanding spark:\n",
    "\n",
    "* uses Resilient Distributed Datasets (RDDs)\n",
    "* processes on a cluster\n",
    "* performs lazy evaluation\n",
    "* multiple interactive shells available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are RDDs?\n",
    "\n",
    "The main abstraction Spark provides is a Resilient Distributed Dataset (RDD), which is a fault-tolerant collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\n",
    "\n",
    "An example of a RDD: \n",
    "\n",
    "```python\n",
    "data = [1, 2, 3, 4, 5]   \n",
    "distData = sc.parallelize(data)\n",
    "```\n",
    "\n",
    "<img src='image/RDD.jpg'/>\n",
    "\n",
    "\n",
    "Our RDD has an array, that is referencing some partions' objects. The idea is that these partion objects are serializable and can be spread around the nodes. For example, if we had a four node cluster, we would expect that each node would be assigned a partion. The partition objects in turn reference memory. The data is kept in RAM on the various nodes (and this is where we will do our operations by default). The reason that Spark is extremely effecient at iterative applications is because RDDs can be cached into memory.\n",
    "    \n",
    "Note: if we had an 1000 node cluster (but only 4 partitions), we would only be using 4 nodes on our cluster (Of course, we want to pay attention to the # of partitions!)\n",
    "\n",
    "Also note that the RDDs are an immutable collection of objects. (Why?) Immutability eliminates potential problems due multiple thread updates. Immutable data is safe to share across processes. \n",
    "\n",
    "### RDD operations\n",
    "\n",
    "There are two main types of operations used by RDDs:\n",
    "\n",
    "1. **transformations**: operations that will be evaluated later on (*lazy*)\n",
    "2. **actions**: when those transformations are actually executed (*executing*)\n",
    "\n",
    "Some examples of each type of operation:\n",
    "\n",
    "Transformations: *lazy* | Actions: *executing*\n",
    "--- | --- \n",
    "**map(func):** pass each element of source through func, return new RDD | **reduce(func):** aggregate elements with func\n",
    "**filter(func):** select elements of the source for which func returns true, return new distributed RDD | **take(n):** copy top n elements to driver\n",
    "**distinct():** return duplicate-free RDD | **collect():** copy all elements to driver\n",
    "**sample(withReplacement, fraction [seed]):** sample RDD, with or without replacement |  **foreach(func):** apply provided func to each element of RDD\n",
    "[more examples](http://spark.apache.org/docs/latest/programming-guide.html#transformations) | [more examples](http://spark.apache.org/docs/latest/programming-guide.html#actions)\n",
    "\n",
    "Below is a command from PySpark with the operation type annotated. Notice the syntax resemblance to Pandas?\n",
    "\n",
    "```bash\n",
    "pagecountsEnAllDF\n",
    "  .select($\"project\", $\"requests\")   //transformation\n",
    "  .groupBy($\"project\")               //transformation\n",
    "  .sum()                             //transformation\n",
    "  .orderBy($\"sum(requests)\".desc)    //transformation\n",
    "  .show()                            //action\n",
    "```\n",
    "\n",
    "If you comment out `show()`, nothing will happen. It won't execute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Clusters (and their components)\n",
    "    \n",
    "Spark applications run as independent sets of processes on a cluster. These processes are coordinated by a spark context (SC) object in the main program (aka driver program).\n",
    "\n",
    "1. In order to run on a cluster, the spark context can connect to several types of cluster managers (Spark's or Mesos/YARN) which allocate resources across applications.                                                                                               \n",
    "2. Spark then acquires executors on nodes in the cluster (executors ~ processes that run computations and store data for your applications.) \n",
    "3. Next, it sends application code (defined by JAR or Python files passed to the SC) to the executors (each application gets its own executor processes!)  \n",
    "8. Finally, the spark context sends tasks for the executors to run (tasks from different applications run in different JVMs!)\n",
    "\n",
    "http://spark.apache.org/docs/latest/cluster-overview.html\n",
    "\n",
    "<img src='image/spark_cluster.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy transformations\n",
    "\n",
    ".. so the results are not computed right away. \n",
    "\n",
    "Spark remembers that transformations were applied to a given dataset. The transformations are only computed when an action requires a result to be returned to the driver program. \n",
    "This design enables Spark to run more efficiently!  \n",
    "\n",
    "For example, imagine a dataset that is called in a mapper.\n",
    "The result of the mapper isnt really required until the reducer is called. Thus, it's more effecient to call the data in the mapper than to call the mapped data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive shells\n",
    "\n",
    "Spark contains a number of interactive shells. We will be covering PySpark today.\n",
    "\n",
    "* `spark-shell` for the Scala shell\n",
    "* `pyspark` for the Python shell"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:scienv3]",
   "language": "python",
   "name": "conda-env-scienv3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
