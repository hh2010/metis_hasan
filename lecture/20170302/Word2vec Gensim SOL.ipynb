{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Word2Vec with Gensim\n",
    "\n",
    "Setup:\n",
    "1. pip install gensim\n",
    "1. cython -V\n",
    "1. (if no cython): pip install cython\n",
    "\n",
    "We have been working with a number of techniques and tools that help us navigate the world of NLP. For example, we have Vectorizers:kljm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>above</th>\n",
       "      <th>above all</th>\n",
       "      <th>all</th>\n",
       "      <th>all to</th>\n",
       "      <th>be</th>\n",
       "      <th>be true</th>\n",
       "      <th>come</th>\n",
       "      <th>come to</th>\n",
       "      <th>denmark</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>the state</th>\n",
       "      <th>thine</th>\n",
       "      <th>thine own</th>\n",
       "      <th>this</th>\n",
       "      <th>this above</th>\n",
       "      <th>to</th>\n",
       "      <th>to thine</th>\n",
       "      <th>to this</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   above  above all  all  all to  be  be true  come  come to  denmark  in  \\\n",
       "0      0          0    0       0   0        0     1        1        0   0   \n",
       "1      1          1    1       1   1        1     0        0        0   0   \n",
       "2      0          0    0       0   0        0     0        0        1   1   \n",
       "\n",
       "   ...   the  the state  thine  thine own  this  this above  to  to thine  \\\n",
       "0  ...     0          0      0          0     1           0   1         0   \n",
       "1  ...     0          0      1          1     1           1   1         1   \n",
       "2  ...     1          1      0          0     0           0   0         0   \n",
       "\n",
       "   to this  true  \n",
       "0        1     0  \n",
       "1        0     1  \n",
       "2        0     0  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "import numpy as np, seaborn as sb\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "text = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "vectorizer.fit(text)\n",
    "x = vectorizer.transform(text)\n",
    "x_back = x.toarray()\n",
    "\n",
    "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with Bag of Words models is context and semantic meaning does not play a role!! This is problematic, because we're humans and we care about context.\n",
    "\n",
    "And then came Word2Vec...\n",
    "\n",
    "Context is hugely important, and [new techniques like t-SNE and Tensorflow](https://www.tensorflow.org/tutorials/word2vec) allow us to visualize these word and phrase relationships.\n",
    "\n",
    "![](https://www.tensorflow.org/images/linear-relationships.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['will', 'this', 'work?', \"i'm\", 'not', 'sure.', 'if', 'not', 'go', 'to', 'step', '#4', '(above)']]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "documents = [\"Will this work?  I'm not sure.  If not go to step #4 (above)\"]\n",
    "texts = [[word for word in document.lower().split()]\n",
    "         for document in documents]\n",
    "\n",
    "print(texts)\n",
    "model = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**4) If you see the following error : \"UserWarning: C extension not loaded for Word2Vec\"**\n",
    "\n",
    "\n",
    "[Do the following](https://groups.google.com/forum/#!topic/gensim/isBqIhrw9mk):\n",
    "\n",
    "1.  pip uninstall gensim\n",
    "2.  pip uninstall scipy \n",
    "3. pip install --no-cache-dir scipy==0.15.1\n",
    "4. pip install --no-cache-dir gensim==0.12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  A 'Gensim' example: \n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Preprocessing:\n",
    "\n",
    "1. Tokenization   \n",
    "2. Remove stop words    \n",
    "3. Convert to lowercase     \n",
    "4. Others: stemming.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The type of input that Word2Vec is looking for.. \n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2: Word Representation\n",
    "\n",
    "Learn a continuous representation of words.\n",
    "Each word (w) is associated with it's own word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model1 = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1, workers=2,sg=1)\n",
    "\n",
    "#print model.vocab\n",
    "#print(model.vocab['minors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00358155,  0.00264495, -0.00401788,  0.00149795, -0.00194208,\n",
       "        0.00045494, -0.00017517,  0.0013563 , -0.00399634,  0.00104367,\n",
       "       -0.00279939,  0.00215701, -0.0022176 , -0.00280289, -0.00152524,\n",
       "        0.00427915,  0.00151263, -0.00368231, -0.00362804,  0.0014829 ,\n",
       "        0.00033702,  0.0014282 , -0.00318798, -0.00481104,  0.00064264,\n",
       "        0.0041865 , -0.00427888,  0.00062682, -0.00080342, -0.00499865,\n",
       "       -0.00364611,  0.00132216, -0.00449747, -0.00371323, -0.00272111,\n",
       "        0.00494667, -0.00280392,  0.00320229,  0.00346256, -0.00225677,\n",
       "       -0.001115  , -0.00231972,  0.00036926, -0.00090166, -0.00046707,\n",
       "       -0.00385928, -0.00196428, -0.00238861, -0.00350803, -0.00146319,\n",
       "        0.00112949,  0.00418342,  0.00376145,  0.00302149,  0.00321145,\n",
       "        0.00430367,  0.00351456,  0.00297585,  0.00080843, -0.00160295,\n",
       "       -0.00343848,  0.00124714,  0.00095836,  0.00302949, -0.00010518,\n",
       "       -0.0009199 , -0.00101466, -0.00157668,  0.00212804,  0.00119765,\n",
       "        0.00340973,  0.00465541,  0.00158798, -0.00401529,  0.00153894,\n",
       "       -0.00358201,  0.00212459,  0.00414292, -0.00073451, -0.0025579 ,\n",
       "        0.00218826, -0.0028648 , -0.00212531,  0.00244971,  0.00201649,\n",
       "        0.00487883,  0.00135356, -0.00052304, -0.00479682, -0.00333597,\n",
       "        0.00125275, -0.00255924,  0.00279517,  0.00099833,  0.00157377,\n",
       "       -0.00353028,  0.00076364,  0.00337701,  0.00400746,  0.00096725], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  and Voila !    We have our word vector \n",
    "model1['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.069379084445707534"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.similarity('trees', 'machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.039476059360845459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.similarity('human', 'machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0032223047000629855"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.similarity('computer', 'system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13791865731732256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.similarity('computer', 'machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.doesnt_match(\"computer human trees\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.doesnt_match(\"computer human machine\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computer'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.doesnt_match(\"computer machine system\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What do we have?   Word Embeddings \n",
    "\n",
    "**A word embedding W : words → ℝn **\n",
    "\n",
    "The output above is the result of 'word' projections in a latent space\n",
    "of N dimensions, (N ~ size of NN layer we chose).     \n",
    "Our float values above represent the coordinates for the word 'computer' in our 100-dimensional space!\n",
    "\n",
    "Our high dimensional vectors stand in place for words.    \n",
    "Note, that these dimensions are encoding 'latent' properties for 'computer' (such that 'queen' will be geometrically closer to 'king' than it would to be to (let's say) 'computer'. \n",
    "\n",
    "\n",
    "Word Embeddings are useful because:\n",
    "\n",
    "1.  We can measure the semantic similarity between two words\n",
    "2.  We can use these word vectors as features for various NLP supervised learning tasks (such as classifcation, sentiment analysis). \n",
    "\n",
    "We will see how we get here.. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![check this](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3:  Skip-Gram Methods &  Continuous Bag of Words (CBOW) Methods : \n",
    "\n",
    "#### Skip-Gram: \n",
    "\n",
    "**example sentence:**  \n",
    "\n",
    "**  \"We are on the cusp of deep learning for the masses\"\n",
    "\n",
    "For Context Window = 2:\n",
    "\n",
    "*We could get the following training examples: (Where target word is in bold) *\n",
    "\n",
    "\n",
    "**We** are on \n",
    "\n",
    "We **are** on the\n",
    "\n",
    "We are **on** the cusp\n",
    "\n",
    "\n",
    "#### What's happening underneath the hood? :\n",
    "for this example : we have input of skip-gram is a single word (Wi) **'learning'**, we will determine the probability of seeing the words (Wo) : 'of','deep', 'for','the'\n",
    "\n",
    "Step 1) Transform our vobabulary into a 'bag of indices'\n",
    "\n",
    "Step 2) [One-hot encode](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) (input vectors) \n",
    "\n",
    "Step 3) Randomly initialize the Weight Vectors\n",
    "\n",
    "Step 4) Get dot product: (Input vector.InputWeightMatrix) ~ (this is just the weight vector for 'learning')\n",
    "\n",
    "Step 5) Get dot product:  ('learning' weight vector).(Output Weight Matrix) \n",
    "\n",
    "Step 6) Calculate Softmax probabilities\n",
    "What is the probability of 'seeing' the word 'deep' given that we've seen the word 'learning'?  -- >  Using SGD together with softmax regression, we will maximize the probability for 'deep' \n",
    "\n",
    "P(Wo|Wi) = (exp(Wi.Wo)/ sum(exp(Wi.Woj)   (sum~ sum of all Woj for all j in Vocabulary)\n",
    "\n",
    "Step 7) As always, we update our Weight matrix to reduce our errors\n",
    "Wi=Wi-a*ej*Wo\n",
    "\n",
    "Step 7) Repeat..\n",
    "\n",
    " \n",
    "<img src='img/skip_gram.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### CBOW: \n",
    "\n",
    "\n",
    "CBOW: very similiar model with the inputs & outputs reversed.  The input layer consists of our word window \n",
    "\n",
    "<img src='img/CBOW.png'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# we may need to download gutenberg\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop+=['?','!','.',',',':',';']\n",
    "\n",
    "#creating our iterator\n",
    "# An Illustration.. \n",
    "\n",
    "import os\n",
    "\n",
    "class MySentences(object):\n",
    "     def __init__(self, dirname):\n",
    "            self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "        # iterate through all file names in our directory\n",
    "         for fname in os.listdir(self.dirname):\n",
    "                for line in open(os.path.join(self.dirname, fname), encoding='utf-8', errors='ignore'):\n",
    "                    word=line.lower().split()\n",
    "                    if word not in stop:\n",
    "                        yield word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jb/anaconda3/lib/python3.5/site-packages/gensim/models/word2vec.py:651: UserWarning: C extension not loaded for Word2Vec, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  warnings.warn(\"C extension not loaded for Word2Vec, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "sentences = MySentences('/Users/jb/nltk_data/corpora/gutenberg/') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences,min_count=1,workers=-1)\n",
    "\n",
    "# looks, like workers = -1, doesnt work for us!\n",
    "#model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5949533581733704),\n",
       " ('captain', 0.5392752289772034),\n",
       " (\"''tis\", 0.5350890159606934),\n",
       " ('sluggard,\"\\'', 0.5228051543235779),\n",
       " ('ahab', 0.5076287984848022),\n",
       " ('huzza!\"', 0.503913402557373),\n",
       " (\"fun?'\", 0.5004797577857971),\n",
       " ('refutations', 0.49884623289108276),\n",
       " (\"what?'\", 0.49297577142715454),\n",
       " (\"'prentice\", 0.487434983253479)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('king' ,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49153613754258013"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity\n",
    "\n",
    "model.similarity('woman','man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59014822458792149"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute cosine_similarity\n",
    "\n",
    "model.n_similarity(['woman', 'girl'], ['man', 'boy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'breakfast'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast soldier cowboy warrior\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"breakfast good lunch dinner\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some things to keep in Mind when using Word2Vec:\n",
    "\n",
    "Word2vec requires a lot of data (100M+ words) to train refined models, or use [pretrained vectors](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)!\n",
    "\n",
    "We don't track where in a window our co-occurant words exist, so if 'New' always appears before 'York' there's a 100% change of 'New' given 'York', but if the window is 3, that doesn't mean any of the slots have a 100% chance of 'New', and it doesn't know of the phrase 'New York'.\n",
    "\n",
    "Doc2Vec method extends the word2vec algorithm to larger blocks of texts (paragraphs, documents, articles):\n",
    "- https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "- http://learningaboutdata.blogspot.com/2014/06/plotting-word-embedding-using-tsne-with.html\n",
    "- https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}